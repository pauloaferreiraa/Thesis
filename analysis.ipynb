{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human activity recognition.\n",
    "The dataset: <http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer#>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_meaning= {1:'still_hands_back',2:'still_hands_side'}\n",
    "\n",
    "def read_data(fname='all'):\n",
    "    print('::::Read Data::::')\n",
    "    freq = '19230U'   # ~19ms, using sampling frequency is 52samples/sec\n",
    "    dt = datetime.now()\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    if fname == 'all':    # read all files\n",
    "        data_files = []\n",
    "        \n",
    "        #for f_id in range(1,16):\n",
    "        data = pd.read_csv('still_accelerometer_pf.csv', \n",
    "                            names=['index','xL', 'yL', 'zL', 'xR', 'yR', 'zR', 'label'], \n",
    "                            header=None, index_col=0)\n",
    "        data.index = pd.date_range(start=dt, periods=data.shape[0], freq=freq)\n",
    "        dt += timedelta(hours=3)\n",
    "        data_files.append(data)\n",
    "            \n",
    "        data = pd.concat(data_files)\n",
    "    \n",
    "    else:    # read one file only\n",
    "        data = pd.read_csv('%s.csv' %fname, \n",
    "                   names=['index','xL', 'yL', 'zL', 'xR', 'yR', 'zR', 'label'], \n",
    "                   header=None, index_col=0)\n",
    "\n",
    "        data.index = pd.date_range(start='00:00:00', periods=data.shape[0], freq=freq)\n",
    "    \n",
    "    # Filter and clean data\n",
    "    data = data.dropna()\n",
    "    data = data[data['label'] != 0]   # some rows are misclassified as 0\n",
    "    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction.\n",
    "Segment the data using sliding window of size 10s, and from each window extract the common features (mean, standard deviation, variance), and some other features used specifically for Human Activity Recognition like RMS (Root Mean Square), magnitude of acceleration, and pairwise correlation between x, y, z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(ts): return np.sqrt(np.mean(ts**2))\n",
    "\n",
    "def corrL(df): \n",
    "    cor = df.corr()\n",
    "    return pd.DataFrame({'xyL':[cor['xL']['yL']], 'xzL':[cor['xL']['zL']], 'yzL':[cor['yL']['zL']]})\n",
    "\n",
    "def corrR(df): \n",
    "    cor = df.corr()\n",
    "    return pd.DataFrame({'xyR':[cor['xR']['yR']], 'xzR':[cor['xR']['zR']], 'yzR':[cor['yR']['zR']]})\n",
    "\n",
    "def get_simple_features(data, wsize='10s', f_list=['mean', 'std', 'var', rms]):\n",
    "    print('::::START:::: Get Simple Features::::')\n",
    "    # f_list is a list of features names or methods to apply in resampling\n",
    "    \n",
    "    # features that invlove one dimension only.\n",
    "    fname = '_' + (f_list[0] if isinstance(f_list[0], str) else f_list[0].__name__)   \n",
    "    feats = data[['xL','yL','zL']].resample(wsize, how=f_list[0]).add_suffix('%s_l' % fname)\n",
    "    feats = data[['xR','yR','zR']].resample(wsize, how=f_list[0]).add_suffix('%s_l' % fname)\n",
    "    \n",
    "    \n",
    "    for i, f in enumerate(f_list[1:]):\n",
    "        fname = '_' + (f if isinstance(f, str) else f.__name__)\n",
    "        feat = data[['xL','yL','zL']].resample(wsize, how=f).add_suffix('%s_L' % fname)\n",
    "        feats = feats.join(feat)\n",
    "        feat = data[['xR','yR','zR']].resample(wsize, how=f).add_suffix('%s_R' % fname)\n",
    "        feats = feats.join(feat) \n",
    "\n",
    "    # features that involve more than one dimension.                                               \n",
    "    mean_mag = (data**2).sum(axis=1).resample(wsize, how=lambda ts: np.sqrt(ts).mean())\n",
    "    mean_mag.name = 'mean_mag'\n",
    "    feats = feats.join(mean_mag) \n",
    "\n",
    "    pairs_cor = data.groupby(pd.TimeGrouper(wsize)).apply(corrL).reset_index(1, drop=True)\n",
    "    feats = feats.join(pairs_cor) \n",
    "    pairs_cor = data.groupby(pd.TimeGrouper(wsize)).apply(corrR).reset_index(1, drop=True)\n",
    "    feats = feats.join(pairs_cor) \n",
    "    \n",
    "    y = data['label'].resample(wsize, how=lambda ts: mode(ts)[0] if ts.shape[0] > 0 else np.nan)   \n",
    "\n",
    "    # drop any nan values\n",
    "    mask = np.any(np.isnan(feats), axis=1)\n",
    "    feats, y = feats[~mask], y[~mask]\n",
    "    mask = np.isnan(y)\n",
    "    feats, y = feats[~mask], y[~mask]\n",
    "    print('::::END:::: Get Simple Features::::')\n",
    "    return (feats, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing and tuning the model.\n",
    "\n",
    "Splitting data (after feature extraction) into train and test subsets. Using grid search on the training set each model is trained and tuned, then the models are compared according to their performance on the test set using accuracy and AUC (Area Under ROC Curve). The best accuracy result of .899 for SVM and RandomForest, and best AUC result of .99 for RandomForest.\n",
    "\n",
    "    Activity codes in confusion matrix:\n",
    "    1: Working at Computer\n",
    "    2: Standing Up, Walking and Going up\\down stairs\n",
    "    3: Standing\n",
    "    4: Walking\n",
    "    5: Going Up\\Down Stairs\n",
    "    6: Walking and Talking with Someone\n",
    "    7: Talking while Standing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, est, grid):\n",
    "    print('::::Train Model::::')\n",
    "    gs = GridSearchCV(estimator=est, param_grid=grid, scoring='accuracy', cv=4, n_jobs=-1)\n",
    "    gs = gs.fit(X, y)\n",
    "    \n",
    "    return (gs.best_estimator_, gs.best_params_)\n",
    "\n",
    "def eval_model(mod, X_test, y_test, mod_name, plt_roc=True):\n",
    "    print('::::Eval Model::::')\n",
    "    y_prob = mod.predict_proba(X_test)\n",
    "    y_test_bin = label_binarize(y_test, classes=[1,2])\n",
    "    \n",
    "    y_test_bin_ravel = y_test_bin.ravel()\n",
    "    y_prob_ravel = y_prob.ravel()\n",
    "    \n",
    "    y_test_bin_ravel = y_test_bin_ravel[:len(y_prob_ravel)]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test_bin_ravel, y_prob_ravel)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    if plt_roc:\n",
    "        plt.plot(fpr, tpr, lw=2,\n",
    "                 label='average ROC curve (auc=%0.2f), model: %s' % (roc_auc,mod_name))\n",
    "        plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    y_pred = mod.predict(X_test)\n",
    "    score = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    print('Accuracy score on the test set: %.3f' %score)\n",
    "    \n",
    "    confusion_ma = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    confusion_ma = pd.DataFrame(confusion_ma, index=list(range(1,7)), columns=list(range(1,7)))\n",
    "    print('Confusion Matrix...')\n",
    "    print(confusion_ma)\n",
    "    \n",
    "    return (roc_auc, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test Example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::::Read Data::::\n",
      "::::START:::: Get Simple Features::::\n",
      "::::END:::: Get Simple Features::::\n",
      "(5, 28)\n",
      "(5,)\n",
      "Support Vector Machine\n",
      "::::Train Model::::\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/paulo/.local/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/home/paulo/.local/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/paulo/.local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/paulo/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/home/paulo/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/home/paulo/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 528, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/paulo/.local/lib/python3.6/site-packages/sklearn/svm/base.py\", line 150, in fit\n    y = self._validate_targets(y)\n  File \"/home/paulo/.local/lib/python3.6/site-packages/sklearn/svm/base.py\", line 525, in _validate_targets\n    \" class\" % len(cls))\nValueError: The number of classes has to be greater than one; got 1 class\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-5dd5a48ca18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m model, params = train_model(X_train, y_train, \n\u001b[1;32m     13\u001b[0m                     \u001b[0mest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     grid={'C': param_range, 'gamma': param_range, 'kernel': ['linear']})\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SVC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-d902442b6ba1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, y, est, grid)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'::::Train Model::::'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "data = read_data('still_accelerometer_pf')\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "feats, y = get_simple_features(data, wsize='10s')\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, y, test_size=.25, random_state=0, stratify=y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Support Vector Machine')\n",
    "model, params = train_model(X_train, y_train, \n",
    "                    est=SVC(probability=True),\n",
    "                    grid={'C': param_range, 'gamma': param_range, 'kernel': ['linear']})\n",
    "eval_model(model, X_test, y_test, 'SVC')\n",
    "              \n",
    "print('Random Forest')\n",
    "model, params = train_model(X_train, y_train, \n",
    "                    est=RandomForestClassifier(n_jobs=-1, criterion='entropy'),\n",
    "                    grid={'n_estimators':[50, 100, 200]})\n",
    "eval_model(model, X_test, y_test,'Forest')\n",
    "\n",
    "print('Gradient Boosting')\n",
    "model, params = train_model(X_train, y_train, \n",
    "                    est=GradientBoostingClassifier(learning_rate=1.0, random_state=0),\n",
    "                    grid={'n_estimators':[50, 200, 400], 'max_depth':[1,2,3],\n",
    "                          'learning_rate':[.1,.05,.01,.005]})\n",
    "eval_model(model, X_test, y_test,'GBC')\n",
    "\n",
    "print('Ada Boosting')\n",
    "model, params = train_model(X_train, y_train, \n",
    "                    est=AdaBoostClassifier(),\n",
    "                    grid={'n_estimators':[50, 200, 400], 'learning_rate':[.1,.05,.01,.005]})\n",
    "eval_model(model, X_test, y_test,'ABC')\n",
    "\n",
    "print('K-Nearest Neighbor')\n",
    "model, params = train_model(X_train, y_train, \n",
    "                    est=KNeighborsClassifier(),\n",
    "                    grid={'n_neighbors':[5, 8, 10], 'weights':['uniform', 'distance']})\n",
    "eval_model(model, X_test, y_test,'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different windows sizes.\n",
    "\n",
    "The choice of a window size of 10sec was quite arbitrary. In the research papers on HAR different window sizes were used that range from 1 sec to 20 sec.\n",
    "Also two types of windows are used in the field, disjoint and overlapping. get_features2 defines the same features defined before but using rolling window and taking only values at overlapping of 50%.\n",
    "AUC scores are comparable, and the best accuracy score is .92 for overlapping-window of size 2sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advanced_features(data, wsize_sec, overlap=.5):\n",
    "    print('::::START:::: Get Advance Features ::::')\n",
    "    \n",
    "    wsize = int(10*wsize_sec)\n",
    "    feats = data[['xL','yL','zL']].rolling(wsize,int(wsize/2)).mean().add_suffix('_mean_l')\n",
    "    feats = feats.join(feat)\n",
    "    feats = data[['xR','yR','zR']].rolling(wsize,int(wsize/2)).mean().add_suffix('_mean_r')\n",
    "    feats = feats.join(feat)\n",
    "    feat = data[['xR','yR','zR']].rolling(wsize, int(wsize/2)).std().add_suffix('_std_r')\n",
    "    feats = feats.join(feat)\n",
    "    feat = data[['xL','yL','zL']].rolling(wsize, int(wsize/2)).std().add_suffix('_std_l')\n",
    "    feats = feats.join(feat)\n",
    "    \n",
    "    feat = data[['xL','yL','zL']].rolling(wsize, int(wsize/2)).var().add_suffix('_var_l')\n",
    "    feats = feats.join(feat)\n",
    "    feat = data[['xR','yR','zR']].rolling(wsize, int(wsize/2)).var().add_suffix('_var_r')\n",
    "    feats = feats.join(feat)\n",
    "    \n",
    "    feat = data[['xL','yL','zL']].rolling(wsize, int(wsize/2)).apply(rms).add_suffix('_rms_l')\n",
    "    feats = feats.join(feat)\n",
    "    feat = data[['xR','yR','zR']].rolling(wsize, int(wsize/2)).apply(rms).add_suffix('_rms_r')\n",
    "    feats = feats.join(feat)\n",
    "        \n",
    "    mean_mag = (data**2).sum(axis=1).rolling(wsize, int(wsize/2)).apply(lambda ts: np.sqrt(ts).mean())\n",
    "    mean_mag.name = 'mean_mag'\n",
    "    feats = feats.join(mean_mag) \n",
    "    \n",
    "    pairs_cor_ = data[['xL','yL','zL']].rolling(window=int(wsize/2)).corr(other=data[['xL','yL','zL']])\n",
    "    feats = feats.join(pairs_cor_)\n",
    "    pairs_cor_ = data[['xR','yR','zR']].rolling(window=int(wsize/2)).corr(other=data[['xR','yR','zR']])\n",
    "    feats = feats.join(pairs_cor_)\n",
    "    \n",
    "    y = data[['label']].rolling(wsize, int(wsize/2)).apply(lambda ts: mode(ts)[0])  \n",
    "    feats = feats.iloc[int(wsize*overlap)::int(wsize*overlap)] \n",
    "    \n",
    "    y = y.iloc[int(wsize*overlap)::int(wsize*overlap)]\n",
    "    print('::::END:::: Get features Advance::::')\n",
    "    \n",
    "    \n",
    "    return feats, y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test Example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "win_sizes = ['2']#,'3', '5', '7', '10', '13', '15', '20']\n",
    "best_model = RandomForestClassifier(criterion='entropy', n_jobs=-1, n_estimators=50)\n",
    "\n",
    "for wsize in win_sizes:\n",
    "    print('Window Size: %s sec' % wsize)\n",
    "    print('Min periodos:', int(wsize)/2)\n",
    "    try:\n",
    "        # disjoint window\n",
    "        print('Disjoint window:')\n",
    "        feats, y = get_simple_features(data, wsize=wsize + 's')\n",
    "               \n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(feats, y, test_size=.25,\n",
    "                                                            random_state=0, stratify=y)\n",
    "        #X_train = X_train.fillna(X_train.mean())\n",
    "        #y_train.fillna(y_train.mean())\n",
    "        \n",
    "        \n",
    "        best_model.fit(X_train, y_train)\n",
    "        roc_auc, acc = eval_model(best_model, X_test, y_test,'%ss - disjoint' %wsize, plt_roc=False)\n",
    "        print('AUC score: %.3f' % roc_auc)\n",
    "\n",
    "\n",
    "        # overlapping window\n",
    "        print('Overlapping window:')\n",
    "        feats, y = get_advanced_features(data, int(wsize))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(feats, y, test_size=.25,\n",
    "                                                            random_state=0, stratify=y)\n",
    "        #X_train = X_train.fillna(X_train.mean())\n",
    "        #y_train.fillna(y_train.mean())\n",
    "        \n",
    "        best_model.fit(X_train, y_train)\n",
    "        roc_auc, acc = eval_model(best_model, X_test, y_test,'%ss - overlapping' %wsize, plt_roc=False)\n",
    "        print('AUC score: %.3f' % roc_auc)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = read_data('all') \n",
    "feats, y = get_advanced_features(data2, 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, y, test_size=.25,\n",
    "                                                    random_state=0, stratify=y)\n",
    "X_train.fillna(X_train.mean())\n",
    "best_model.fit(X_train, y_train)\n",
    "eval_model(best_model, X_test, y_test,'2sec - overlapping')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
